{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTlo1MReaKvH",
        "outputId": "8886dcf4-6cb3-464c-b5d8-2788bb91580e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            " ‚ùåüî¥‚ùåüî¥‚ùåüî¥‚ùåüî¥ BEM-VINDO AO JOGO QUIXO ‚ùåüî¥‚ùåüî¥‚ùåüî¥‚ùåüî¥ \n",
            "\n",
            "\n",
            "==================================================\n",
            "1. üë§ Jogar contra Minimax\n",
            "2. ü§ñ Jogar contra Q-Learning\n",
            "3. üß† Treinar Q-Learning\n",
            "4. üîÑ Q-Learning vs Minimax\n",
            "5. üìä Avaliar modelos\n",
            "6. üíæ Carregar modelo Q-Learning\n",
            "7. ‚ùì Ajuda\n",
            "8. üö™ Sair\n",
            "==================================================\n",
            "Escolha uma op√ß√£o: 3\n",
            "\n",
            "üß† CONFIGURA√á√ÉO DO TREINAMENTO\n",
            "========================================\n",
            "\n",
            "\n",
            "üëã Programa encerrado pelo usu√°rio.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import asyncio\n",
        "import threading\n",
        "import time\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dicion√°rios de apoio\n",
        "\n",
        "# Dicionario de posi√ß√µes correpondentes do tabuleiro no array\n",
        "dicio = {\n",
        "    '1,1': 0, '1,2': 1, '1,3': 2, '1,4': 3, '1,5': 4,\n",
        "    '2,1': 5, '2,2': 6, '2,3': 7, '2,4': 8, '2,5': 9,\n",
        "    '3,1': 10, '3,2': 11, '3,3': 12, '3,4': 13, '3,5': 14,\n",
        "    '4,1': 15, '4,2': 16, '4,3': 17, '4,4': 18, '4,5': 19,\n",
        "    '5,1': 20, '5,2': 21, '5,3': 22, '5,4': 23, '5,5': 24\n",
        "}\n",
        "\n",
        "# Posi√ß√µes inversas para traduzir √≠ndices do array para coordenadas do tabuleiro\n",
        "dicio_inverso = {v: k for k, v in dicio.items()}\n",
        "\n",
        "# Posi√ß√µes na borda (as √∫nicas que podem ser movidas de acordo com as regras do Quixo)\n",
        "posicoes_borda = np.array([\n",
        "    '1,1', '1,2', '1,3', '1,4', '1,5',\n",
        "    '2,1', '2,5', '3,1', '3,5',\n",
        "    '4,1', '4,5', '5,1', '5,2', '5,3', '5,4', '5,5'\n",
        "])\n",
        "\n",
        "\n",
        "# Indices correspondentes a cada do tabuleiro no array\n",
        "dicioLinhas = {\n",
        "    'Linha 1': [0, 1, 2, 3, 4], 'Linha 2': [5, 6, 7, 8, 9], 'Linha 3': [10, 11, 12, 13, 14],\n",
        "    'Linha 4': [15, 16, 17, 18, 19], 'Linha 5': [20, 21, 22, 23, 24],\n",
        "    'Coluna 1': [0, 5, 10, 15, 20], 'Coluna 2': [1, 6, 11, 16, 21],\n",
        "    'Coluna 3': [2, 7, 12, 17, 22], 'Coluna 4': [3, 8, 13, 18, 23],\n",
        "    'Coluna 5': [4, 9, 14, 19, 24], 'Diagonal 1': [0, 6, 12, 18, 24],\n",
        "    'Diagonal 2': [4, 8, 12, 16, 20]\n",
        "}\n",
        "\n",
        "# Classe para armazenar o hist√≥rico de jogadas\n",
        "class HistoricoJogadas:\n",
        "    def __init__(self, max_jogadas=10):\n",
        "        self.jogadas = []\n",
        "        self.max_jogadas = max_jogadas\n",
        "\n",
        "    def adicionar_jogada(self, jogador, origem, destino):\n",
        "        #Marca hora em que a jogada ocorreu\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        jogada = {\n",
        "            \"jogador\": jogador,\n",
        "            \"origem\": origem,\n",
        "            \"destino\": destino,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "        self.jogadas.append(jogada)\n",
        "        if len(self.jogadas) > self.max_jogadas:\n",
        "            self.jogadas.pop(0)\n",
        "\n",
        "    def obter_ultimas_jogadas(self, quantidade=None):\n",
        "        if quantidade is None or quantidade > len(self.jogadas):\n",
        "            return self.jogadas\n",
        "        return self.jogadas[-quantidade:]\n",
        "\n",
        "    def imprimir_historico(self):\n",
        "        print(\"\\n=== HIST√ìRICO DE JOGADAS ===\")\n",
        "        for i, jogada in enumerate(self.jogadas, 1):\n",
        "            print(f\"{i}. {jogada['jogador']} moveu de {jogada['origem']} para {jogada['destino']} ({jogada['timestamp']})\")\n",
        "        print(\"===========================\\n\")\n",
        "\n",
        "# Classe do jogo\n",
        "class Jogo:\n",
        "    def __init__(self, tabuleiro=None, jogador='üî¥'):\n",
        "        #Cria√ß√£o do tabuleiro\n",
        "        self.tabuleiro = np.full(25, \"‚¨ú\", dtype=str) if tabuleiro is None else np.array(tabuleiro, dtype=str)\n",
        "        self.jogador_atual = jogador\n",
        "        self.historico = HistoricoJogadas()\n",
        "        self.ultima_jogada_agente = None\n",
        "\n",
        "    def turno(self):\n",
        "        return self.jogador_atual\n",
        "\n",
        "    def codificar_estado(self):\n",
        "        # Codifica o estado do jogo como uma string para usar como chave na tabela Q\n",
        "        estado_encoded = \"\"\n",
        "        for peca in self.tabuleiro:\n",
        "            if peca == \"‚¨ú\":\n",
        "                estado_encoded += \"0\"\n",
        "            elif peca == \"üî¥\":\n",
        "                estado_encoded += \"1\"\n",
        "            elif peca == \"‚ùå\":\n",
        "                estado_encoded += \"2\"\n",
        "        return estado_encoded + \"_\" + (\"1\" if self.jogador_atual == \"üî¥\" else \"2\")\n",
        "\n",
        "    # Verifica quem venceu\n",
        "    def venceu(self):\n",
        "        for nome, indices in dicioLinhas.items():\n",
        "            valores = [self.tabuleiro[i] for i in indices]\n",
        "            if all(v == \"‚ùå\" for v in valores) or all(v == \"üî¥\" for v in valores):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Verifica se empatou\n",
        "    def empate(self):\n",
        "        return not self.venceu() and \"‚¨ú\" not in self.tabuleiro\n",
        "\n",
        "    # Calcula a fun√ß√£o de utilidade para saber o melhor estado a seguir\n",
        "    def calcular_utilidade(self, jogador):\n",
        "        if self.venceu():\n",
        "            return 1 if self.jogador_atual == jogador else -1\n",
        "\n",
        "        pontos = 0\n",
        "        for nome, indices in dicioLinhas.items():\n",
        "            valores = [self.tabuleiro[i] for i in indices]\n",
        "            if jogador == \"‚ùå\":\n",
        "                pontos += valores.count(\"‚ùå\") * 0.1\n",
        "                pontos -= valores.count(\"üî¥\") * 0.1\n",
        "            else:\n",
        "                pontos += valores.count(\"üî¥\") * 0.1\n",
        "                pontos -= valores.count(\"‚ùå\") * 0.1\n",
        "\n",
        "        return pontos\n",
        "\n",
        "    # Gera todos os mov v√°lidos poss√≠veis para o jogador atual\n",
        "    def jogos_validos(self):\n",
        "        filhos = [] # Armazena todas as jogadas v√°lidas poss√≠veis a partir do estado atual\n",
        "        for pos in posicoes_borda: # Itera pelas posi√ß√µes da borda\n",
        "            i = dicio[pos] # Encontra indice correspondente a coordenada\n",
        "            if self.tabuleiro[i] == \"‚¨ú\" or self.tabuleiro[i] == self.jogador_atual: # Verifica se √© uma pe√ßa v√°lida de ser jogada\n",
        "                for destino in self.movimentos_validos(pos): # Encontra destinos v√°lidos para a posi√ß√£o\n",
        "                    novo = self.jogar((pos, destino)) # Simula a jogada e verifica se ela √© v√°lida\n",
        "                    if novo:\n",
        "                        filhos.append((pos, destino))\n",
        "        return filhos # Retorna jogadas v√°lidas\n",
        "\n",
        "    #Fazer a jogada\n",
        "    def jogar(self, movimento):\n",
        "        origem_str, destino_str = movimento\n",
        "        origem_i, origem_j = map(int, origem_str.split(','))\n",
        "        destino_i, destino_j = map(int, destino_str.split(','))\n",
        "\n",
        "        if origem_str not in dicio or destino_str not in dicio:\n",
        "            return None\n",
        "\n",
        "        origem = dicio[origem_str]\n",
        "        destino = dicio[destino_str]\n",
        "\n",
        "        if origem_str not in posicoes_borda:\n",
        "            return None\n",
        "\n",
        "        if self.tabuleiro[origem] != \"‚¨ú\" and self.tabuleiro[origem] != self.jogador_atual:\n",
        "            return None\n",
        "\n",
        "        novo_tabuleiro = np.array(self.tabuleiro)\n",
        "        if origem_i == destino_i:  # linha\n",
        "            linha = origem_i\n",
        "            indices = [dicio[f\"{linha},{j}\"] for j in range(1, 6)]\n",
        "            if destino_j > origem_j:\n",
        "                for k in range(0, 4):\n",
        "                    novo_tabuleiro[indices[k]] = novo_tabuleiro[indices[k + 1]]\n",
        "                novo_tabuleiro[indices[4]] = self.jogador_atual\n",
        "            else:\n",
        "                for k in range(4, 0, -1):\n",
        "                    novo_tabuleiro[indices[k]] = novo_tabuleiro[indices[k - 1]]\n",
        "                novo_tabuleiro[indices[0]] = self.jogador_atual\n",
        "        elif origem_j == destino_j:  # coluna\n",
        "            coluna = origem_j\n",
        "            indices = [dicio[f\"{i},{coluna}\"] for i in range(1, 6)]\n",
        "            if destino_i > origem_i:\n",
        "                for k in range(0, 4):\n",
        "                    novo_tabuleiro[indices[k]] = novo_tabuleiro[indices[k + 1]]\n",
        "                novo_tabuleiro[indices[4]] = self.jogador_atual\n",
        "            else:\n",
        "                for k in range(4, 0, -1):\n",
        "                    novo_tabuleiro[indices[k]] = novo_tabuleiro[indices[k - 1]]\n",
        "                novo_tabuleiro[indices[0]] = self.jogador_atual\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        proximo_jogador = \"üî¥\" if self.jogador_atual == \"‚ùå\" else \"‚ùå\"\n",
        "        novo_jogo = Jogo(novo_tabuleiro, proximo_jogador)\n",
        "        novo_jogo.historico = self.historico\n",
        "        return novo_jogo\n",
        "\n",
        "    def movimentos_validos(self, origem_str):\n",
        "        origem_i, origem_j = map(int, origem_str.split(','))\n",
        "        destinos = []\n",
        "\n",
        "        if origem_i == 1:\n",
        "            destinos.append(f\"5,{origem_j}\")\n",
        "        elif origem_i == 5:\n",
        "            destinos.append(f\"1,{origem_j}\")\n",
        "\n",
        "        if origem_j == 1:\n",
        "            destinos.append(f\"{origem_i},5\")\n",
        "        elif origem_j == 5:\n",
        "            destinos.append(f\"{origem_i},1\")\n",
        "\n",
        "        return destinos\n",
        "\n",
        "    #Imprime tabuleiro\n",
        "    def imprimir(self):\n",
        "        return print(\"| \" + self.tabuleiro[0] + \" | \" + self.tabuleiro[1] + \" | \" + self.tabuleiro[2]+ \" | \" + self.tabuleiro[3] + \" | \" + self.tabuleiro[4] + \" | \" + \"\\n\" +\n",
        "        \"| \" + self.tabuleiro[5] + \" | \" + self.tabuleiro[6] + \" | \" + self.tabuleiro[7]+ \" | \" + self.tabuleiro[8] + \" | \" + self.tabuleiro[9] + \" | \" + \"\\n\" +\n",
        "        \"| \" + self.tabuleiro[10] + \" | \" + self.tabuleiro[11] + \" | \" + self.tabuleiro[12]+ \" | \" + self.tabuleiro[13] + \" | \" + self.tabuleiro[14] + \" | \" + \"\\n\" +\n",
        "        \"| \" + self.tabuleiro[15] + \" | \" + self.tabuleiro[16] + \" | \" + self.tabuleiro[17]+ \" | \" + self.tabuleiro[18] + \" | \" + self.tabuleiro[19] + \" | \" + \"\\n\" +\n",
        "        \"| \" + self.tabuleiro[20] + \" | \" + self.tabuleiro[21] + \" | \" + self.tabuleiro[22]+ \" | \" + self.tabuleiro[23] + \" | \" + self.tabuleiro[24] + \" | \" + \"\\n\")\n",
        "\n",
        "# Classe para agente Q-Learning\n",
        "class QLearningAgent:\n",
        "    def __init__(self, jogador=\"‚ùå\", alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.jogador = jogador\n",
        "        self.alpha = alpha  # Taxa de aprendizagem\n",
        "        self.gamma = gamma  # Fator de desconto\n",
        "        self.epsilon = epsilon  # Taxa de explora√ß√£o\n",
        "        self.epsilon_decay = epsilon_decay  # Decaimento da taxa de explora√ß√£o\n",
        "        self.min_epsilon = min_epsilon  # Taxa m√≠nima de explora√ß√£o\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))  # Tabela Q: {estado: {a√ß√£o: valor}}\n",
        "        self.historico_estados = []  # Para aprendizagem temporal\n",
        "        self.historico_acoes = []\n",
        "        self.historico_recompensas = []\n",
        "\n",
        "    def codificar_acao(self, acao):\n",
        "        # Codifica uma a√ß√£o (tupla de origem e destino) como string\n",
        "        return f\"{acao[0]}_{acao[1]}\"\n",
        "\n",
        "    def decodificar_acao(self, acao_str):\n",
        "        # Decodifica uma string de a√ß√£o para tupla\n",
        "        origem, destino = acao_str.split('_')\n",
        "        return (origem, destino)\n",
        "\n",
        "    def escolher_acao(self, jogo, modo=\"treino\"):\n",
        "        # Escolhe uma a√ß√£o usando epsilon-greedy ou modo de teste\n",
        "        estado = jogo.codificar_estado()\n",
        "        acoes_validas = jogo.jogos_validos()\n",
        "\n",
        "        if not acoes_validas:\n",
        "            return None\n",
        "\n",
        "        # Modo de teste: sempre escolhe a melhor a√ß√£o conhecida\n",
        "        if modo == \"teste\":\n",
        "            melhor_acao = None\n",
        "            melhor_valor = float('-inf')\n",
        "\n",
        "            for acao in acoes_validas:\n",
        "                acao_str = self.codificar_acao(acao)\n",
        "                valor = self.q_table[estado][acao_str]\n",
        "                if valor > melhor_valor:\n",
        "                    melhor_valor = valor\n",
        "                    melhor_acao = acao\n",
        "\n",
        "            return melhor_acao if melhor_acao else random.choice(acoes_validas)\n",
        "\n",
        "        # Modo de treino: epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            # Explora√ß√£o: a√ß√£o aleat√≥ria\n",
        "            return random.choice(acoes_validas)\n",
        "        else:\n",
        "            # Explora√ß√£o: melhor a√ß√£o conhecida\n",
        "            melhor_acao = None\n",
        "            melhor_valor = float('-inf')\n",
        "\n",
        "            for acao in acoes_validas:\n",
        "                acao_str = self.codificar_acao(acao)\n",
        "                valor = self.q_table[estado][acao_str]\n",
        "                if valor > melhor_valor:\n",
        "                    melhor_valor = valor\n",
        "                    melhor_acao = acao\n",
        "\n",
        "            return melhor_acao if melhor_acao else random.choice(acoes_validas)\n",
        "\n",
        "    def calcular_recompensa(self, jogo_anterior, jogo_atual, acao):\n",
        "        # Calcula a recompensa para uma a√ß√£o espec√≠fica\n",
        "        # Recompensa b√°sica por vit√≥ria/derrota\n",
        "        if jogo_atual.venceu():\n",
        "            if jogo_anterior.jogador_atual == self.jogador:\n",
        "                return 100  # Vit√≥ria\n",
        "            else:\n",
        "                return -100  # Derrota\n",
        "\n",
        "        if jogo_atual.empate():\n",
        "            return 0  # Empate\n",
        "\n",
        "        # Recompensas intermedi√°rias baseadas na posi√ß√£o estrat√©gica\n",
        "        recompensa = 0\n",
        "\n",
        "        # Analisa vantagens posicionais\n",
        "        for nome, indices in dicioLinhas.items():\n",
        "            valores_anterior = [jogo_anterior.tabuleiro[i] for i in indices]\n",
        "            valores_atual = [jogo_atual.tabuleiro[i] for i in indices]\n",
        "\n",
        "            # Conta pe√ßas do jogador em cada linha/coluna/diagonal\n",
        "            pecas_anteriores = valores_anterior.count(self.jogador)\n",
        "            pecas_atuais = valores_atual.count(self.jogador)\n",
        "\n",
        "            # Recompensa por formar sequ√™ncias\n",
        "            if pecas_atuais > pecas_anteriores:\n",
        "                recompensa += (pecas_atuais ** 2) * 2  # Recompensa exponencial por sequ√™ncias\n",
        "\n",
        "            # Penaliza por permitir sequ√™ncias do oponente\n",
        "            oponente = \"üî¥\" if self.jogador == \"‚ùå\" else \"‚ùå\"\n",
        "            pecas_oponente_anterior = valores_anterior.count(oponente)\n",
        "            pecas_oponente_atual = valores_atual.count(oponente)\n",
        "\n",
        "            if pecas_oponente_atual > pecas_oponente_anterior:\n",
        "                recompensa -= (pecas_oponente_atual ** 2) * 1.5\n",
        "\n",
        "        return recompensa\n",
        "\n",
        "    def atualizar_q_table(self, estado, acao, recompensa, proximo_estado):\n",
        "        # Atualiza a tabela Q usando a equa√ß√£o do Q-Learning\n",
        "        acao_str = self.codificar_acao(acao)\n",
        "\n",
        "        # Q(s,a) = Q(s,a) + Œ±[r + Œ≥ * max Q(s',a') - Q(s,a)]\n",
        "        q_atual = self.q_table[estado][acao_str]\n",
        "\n",
        "        # Encontra o valor m√°ximo Q para o pr√≥ximo estado\n",
        "        max_q_proximo = 0\n",
        "        if proximo_estado in self.q_table:\n",
        "            max_q_proximo = max(self.q_table[proximo_estado].values()) if self.q_table[proximo_estado] else 0\n",
        "\n",
        "        # Atualiza√ß√£o da tabela Q\n",
        "        novo_q = q_atual + self.alpha * (recompensa + self.gamma * max_q_proximo - q_atual)\n",
        "        self.q_table[estado][acao_str] = novo_q\n",
        "\n",
        "    def treinar_episodio(self, oponente_type=\"random\"):\n",
        "        # Treina um epis√≥dio completo contra um oponente\n",
        "        jogo = Jogo(jogador=self.jogador)\n",
        "        historico_experiencias = []  # (estado, a√ß√£o, recompensa, pr√≥ximo_estado)\n",
        "\n",
        "        max_jogadas = 50  # Evita jogos infinitos\n",
        "        contador = 0\n",
        "\n",
        "        while not jogo.venceu() and not jogo.empate() and contador < max_jogadas:\n",
        "            if jogo.turno() == self.jogador:\n",
        "                # Turno do agente Q-Learning\n",
        "                estado_atual = jogo.codificar_estado()\n",
        "                acao = self.escolher_acao(jogo, modo=\"treino\")\n",
        "\n",
        "                if acao is None:\n",
        "                    break\n",
        "\n",
        "                jogo_anterior = jogo\n",
        "                jogo = jogo.jogar(acao)\n",
        "\n",
        "                if jogo is None:\n",
        "                    break\n",
        "\n",
        "                # Calcula recompensa e armazena experi√™ncia\n",
        "                recompensa = self.calcular_recompensa(jogo_anterior, jogo, acao)\n",
        "                proximo_estado = jogo.codificar_estado()\n",
        "\n",
        "                historico_experiencias.append((estado_atual, acao, recompensa, proximo_estado))\n",
        "\n",
        "            else:\n",
        "                # Turno do oponente\n",
        "                if oponente_type == \"random\":\n",
        "                    acoes_validas = jogo.jogos_validos()\n",
        "                    if acoes_validas:\n",
        "                        acao = random.choice(acoes_validas)\n",
        "                        jogo = jogo.jogar(acao)\n",
        "                elif oponente_type == \"minimax\":\n",
        "                    acao = melhor_jogada(jogo, profundidade=2)\n",
        "                    if acao:\n",
        "                        jogo = jogo.jogar(acao)\n",
        "\n",
        "                if jogo is None:\n",
        "                    break\n",
        "\n",
        "            contador += 1\n",
        "\n",
        "        # Atualiza a tabela Q com todas as experi√™ncias do epis√≥dio\n",
        "        for estado, acao, recompensa, proximo_estado in historico_experiencias:\n",
        "            self.atualizar_q_table(estado, acao, recompensa, proximo_estado)\n",
        "\n",
        "        # Decai epsilon\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # Retorna resultado do jogo\n",
        "        if jogo.venceu():\n",
        "            return \"vitoria\" if contador > 0 and historico_experiencias else \"derrota\"\n",
        "        else:\n",
        "            return \"empate\"\n",
        "\n",
        "    def salvar_modelo(self, nome_arquivo=\"q_learning_model.pkl\"):\n",
        "        # Salva o modelo Q-Learning\n",
        "        dados = {\n",
        "            'q_table': dict(self.q_table),\n",
        "            'epsilon': self.epsilon,\n",
        "            'jogador': self.jogador,\n",
        "            'alpha': self.alpha,\n",
        "            'gamma': self.gamma\n",
        "        }\n",
        "        with open(nome_arquivo, 'wb') as f:\n",
        "            pickle.dump(dados, f)\n",
        "\n",
        "    def carregar_modelo(self, nome_arquivo=\"q_learning_model.pkl\"):\n",
        "        # Carrega um modelo Q-Learning\n",
        "        try:\n",
        "            with open(nome_arquivo, 'rb') as f:\n",
        "                dados = pickle.load(f)\n",
        "\n",
        "            self.q_table = defaultdict(lambda: defaultdict(float), dados['q_table'])\n",
        "            self.epsilon = dados['epsilon']\n",
        "            self.jogador = dados['jogador']\n",
        "            self.alpha = dados['alpha']\n",
        "            self.gamma = dados['gamma']\n",
        "            return True\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Arquivo {nome_arquivo} n√£o encontrado.\")\n",
        "            return False\n",
        "\n",
        "# Classe para treinamento e avalia√ß√£o\n",
        "class TreinadorQLearning:\n",
        "    def __init__(self, agente):\n",
        "        self.agente = agente\n",
        "        self.metricas = {\n",
        "            'vitorias': 0,\n",
        "            'derrotas': 0,\n",
        "            'empates': 0,\n",
        "            'episodios': 0\n",
        "        }\n",
        "        self.historico_metricas = []\n",
        "\n",
        "    def treinar(self, num_episodios=10000, oponente=\"random\", salvar_a_cada=1000, nome_arquivo=\"q_learning_model.pkl\"):\n",
        "        # Treina o agente por um n√∫mero espec√≠fico de epis√≥dios\n",
        "        print(f\"ü§ñ Iniciando treinamento por {num_episodios} epis√≥dios contra oponente {oponente}\")\n",
        "        print(f\"üíæ Salvando modelo a cada {salvar_a_cada} epis√≥dios\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episodio in range(1, num_episodios + 1):\n",
        "            resultado = self.agente.treinar_episodio(oponente_type=oponente)\n",
        "\n",
        "            # Atualiza m√©tricas\n",
        "            if resultado == \"vitoria\":\n",
        "                self.metricas['vitorias'] += 1\n",
        "            elif resultado == \"derrota\":\n",
        "                self.metricas['derrotas'] += 1\n",
        "            else:\n",
        "                self.metricas['empates'] += 1\n",
        "\n",
        "            self.metricas['episodios'] = episodio\n",
        "\n",
        "            # Relat√≥rio de progresso\n",
        "            if episodio % 100 == 0:\n",
        "                taxa_vitoria = (self.metricas['vitorias'] / episodio) * 100\n",
        "                print(f\"Epis√≥dio {episodio}/{num_episodios} - \"\n",
        "                      f\"Vit√≥rias: {taxa_vitoria:.1f}% - \"\n",
        "                      f\"Epsilon: {self.agente.epsilon:.3f}\")\n",
        "\n",
        "            # Salva modelo periodicamente\n",
        "            if episodio % salvar_a_cada == 0:\n",
        "                self.agente.salvar_modelo(nome_arquivo)\n",
        "                print(f\"üíæ Modelo salvo no epis√≥dio {episodio}\")\n",
        "\n",
        "                # Armazena m√©tricas hist√≥ricas\n",
        "                self.historico_metricas.append({\n",
        "                    'episodio': episodio,\n",
        "                    'vitorias': self.metricas['vitorias'],\n",
        "                    'derrotas': self.metricas['derrotas'],\n",
        "                    'empates': self.metricas['empates'],\n",
        "                    'taxa_vitoria': (self.metricas['vitorias'] / episodio) * 100,\n",
        "                    'epsilon': self.agente.epsilon\n",
        "                })\n",
        "\n",
        "        # Salva modelo final\n",
        "        self.agente.salvar_modelo(nome_arquivo)\n",
        "\n",
        "        tempo_total = time.time() - start_time\n",
        "        print(f\"\\nüèÅ Treinamento conclu√≠do em {tempo_total:.2f} segundos\")\n",
        "        self.imprimir_relatorio_final()\n",
        "\n",
        "    def imprimir_relatorio_final(self):\n",
        "        # Imprime relat√≥rio final do treinamento com resultados\n",
        "        total = self.metricas['episodios']\n",
        "        print(f\"\\n=== RELAT√ìRIO FINAL DO TREINAMENTO ===\")\n",
        "        print(f\"Total de epis√≥dios: {total}\")\n",
        "        print(f\"Vit√≥rias: {self.metricas['vitorias']} ({(self.metricas['vitorias']/total)*100:.1f}%)\")\n",
        "        print(f\"Derrotas: {self.metricas['derrotas']} ({(self.metricas['derrotas']/total)*100:.1f}%)\")\n",
        "        print(f\"Empates: {self.metricas['empates']} ({(self.metricas['empates']/total)*100:.1f}%)\")\n",
        "        print(f\"Epsilon final: {self.agente.epsilon:.4f}\")\n",
        "        print(f\"Tamanho da tabela Q: {len(self.agente.q_table)} estados\")\n",
        "        print(\"=====================================\\n\")\n",
        "\n",
        "    def avaliar_contra_minimax(self, num_jogos=100):\n",
        "        # Avalia o agente Q-Learning contra o Minimax\n",
        "        print(f\"üîÑ Avaliando Q-Learning vs Minimax em {num_jogos} jogos...\")\n",
        "\n",
        "        vitorias_q = 0\n",
        "        vitorias_minimax = 0\n",
        "        empates = 0\n",
        "\n",
        "        for jogo_idx in range(num_jogos):\n",
        "            # Alterna quem come√ßa\n",
        "            if jogo_idx % 2 == 0:\n",
        "                resultado = self._jogar_q_vs_minimax(q_comeca=True)\n",
        "            else:\n",
        "                resultado = self._jogar_q_vs_minimax(q_comeca=False)\n",
        "\n",
        "            if resultado == \"q_venceu\":\n",
        "                vitorias_q += 1\n",
        "            elif resultado == \"minimax_venceu\":\n",
        "                vitorias_minimax += 1\n",
        "            else:\n",
        "                empates += 1\n",
        "\n",
        "            if (jogo_idx + 1) % 20 == 0:\n",
        "                print(f\"Progresso: {jogo_idx + 1}/{num_jogos} jogos\")\n",
        "\n",
        "        print(f\"\\n=== RESULTADO Q-LEARNING vs MINIMAX ===\")\n",
        "        print(f\"Q-Learning: {vitorias_q} vit√≥rias ({(vitorias_q/num_jogos)*100:.1f}%)\")\n",
        "        print(f\"Minimax: {vitorias_minimax} vit√≥rias ({(vitorias_minimax/num_jogos)*100:.1f}%)\")\n",
        "        print(f\"Empates: {empates} ({(empates/num_jogos)*100:.1f}%)\")\n",
        "        print(\"======================================\\n\")\n",
        "\n",
        "        return {\n",
        "            'q_learning': vitorias_q,\n",
        "            'minimax': vitorias_minimax,\n",
        "            'empates': empates,\n",
        "            'total': num_jogos\n",
        "        }\n",
        "\n",
        "    def _jogar_q_vs_minimax(self, q_comeca=True):\n",
        "        # Joga uma partida entre Q-Learning e Minimax\n",
        "        jogo = Jogo(jogador=\"üî¥\" if q_comeca else \"‚ùå\")\n",
        "        max_jogadas = 50\n",
        "        contador = 0\n",
        "\n",
        "        while not jogo.venceu() and not jogo.empate() and contador < max_jogadas:\n",
        "            if (q_comeca and jogo.turno() == \"üî¥\") or (not q_comeca and jogo.turno() == \"‚ùå\"):\n",
        "                # Turno do Q-Learning\n",
        "                acao = self.agente.escolher_acao(jogo, modo=\"teste\")\n",
        "                if acao:\n",
        "                    jogo = jogo.jogar(acao)\n",
        "            else:\n",
        "                # Turno do Minimax\n",
        "                acao = melhor_jogada(jogo, profundidade=2)\n",
        "                if acao:\n",
        "                    jogo = jogo.jogar(acao)\n",
        "\n",
        "            if jogo is None:\n",
        "                break\n",
        "\n",
        "            contador += 1\n",
        "\n",
        "        if jogo and jogo.venceu():\n",
        "            # Determina quem venceu baseado no √∫ltimo jogador\n",
        "            ultimo_jogador = \"‚ùå\" if jogo.turno() == \"üî¥\" else \"üî¥\"\n",
        "            if (q_comeca and ultimo_jogador == \"üî¥\") or (not q_comeca and ultimo_jogador == \"‚ùå\"):\n",
        "                return \"q_venceu\"\n",
        "            else:\n",
        "                return \"minimax_venceu\"\n",
        "        else:\n",
        "            return \"empate\"\n",
        "\n",
        "# Minimax com poda alfa-beta\n",
        "def minimax(jogo, turno_max, jogador, profundidade_max=3, alfa=float(\"-inf\"), beta=float(\"inf\")):\n",
        "    if jogo.venceu() or jogo.empate() or profundidade_max == 0:\n",
        "        return jogo.calcular_utilidade(jogador)\n",
        "\n",
        "    if turno_max:\n",
        "        melhor = float(\"-inf\")\n",
        "        for movimento in jogo.jogos_validos():\n",
        "            resultado = jogo.jogar(movimento)\n",
        "            if resultado:\n",
        "                valor = minimax(resultado, False, jogador, profundidade_max - 1, alfa, beta)\n",
        "                melhor = max(melhor, valor)\n",
        "                alfa = max(alfa, melhor)\n",
        "                if beta <= alfa:\n",
        "                    break\n",
        "        return melhor\n",
        "    else:\n",
        "        pior = float(\"inf\")\n",
        "        for movimento in jogo.jogos_validos():\n",
        "            resultado = jogo.jogar(movimento)\n",
        "            if resultado:\n",
        "                valor = minimax(resultado, True, jogador, profundidade_max - 1, alfa, beta)\n",
        "                pior = min(pior, valor)\n",
        "                beta = min(beta, pior)\n",
        "                if beta <= alfa:\n",
        "                    break\n",
        "        return pior\n",
        "\n",
        "def melhor_jogada(jogo, profundidade=2):\n",
        "    movimentos = jogo.jogos_validos()\n",
        "    if not movimentos:\n",
        "        return None\n",
        "\n",
        "    if random.random() < 0.2:\n",
        "        return random.choice(movimentos)\n",
        "\n",
        "    if jogo.ultima_jogada_agente in movimentos and len(movimentos) > 1:\n",
        "        movimentos.remove(jogo.ultima_jogada_agente)\n",
        "\n",
        "    melhor_valor = float(\"-inf\")\n",
        "    melhores_movimentos = []\n",
        "\n",
        "    for movimento in movimentos:\n",
        "        resultado = jogo.jogar(movimento)\n",
        "        if resultado:\n",
        "            valor = minimax(resultado, False, jogo.turno(), profundidade)\n",
        "            if valor > melhor_valor:\n",
        "                melhor_valor = valor\n",
        "                melhores_movimentos = [movimento]\n",
        "            elif valor == melhor_valor:\n",
        "                melhores_movimentos.append(movimento)\n",
        "\n",
        "    return random.choice(melhores_movimentos) if melhores_movimentos else movimentos[0]\n",
        "\n",
        "# Fun√ß√µes de interface\n",
        "def mostrar_ajuda():\n",
        "    print(\"\\n=== REGRAS DO QUIXO ===\")\n",
        "    print(\"1. O tabuleiro √© 5x5\")\n",
        "    print(\"2. Objetivo: formar uma linha de 5 pe√ßas do seu s√≠mbolo (horizontal, vertical ou diagonal)\")\n",
        "    print(\"3. Regras de movimento:\")\n",
        "    print(\"   - S√≥ pode mover pe√ßas da borda\")\n",
        "    print(\"   - S√≥ pode mover pe√ßas vazias ou suas pe√ßas\")\n",
        "    print(\"   - A pe√ßa √© removida e empurrada do lado oposto\")\n",
        "    print(\"   - Sua pe√ßa sempre fica do lado para onde empurrou\")\n",
        "    print(\"4. Formato da jogada: origem destino (ex: 1,1 5,1)\")\n",
        "    print(\"======================\\n\")\n",
        "\n",
        "def menu_principal():\n",
        "    # Menu principal do jogo\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\n\\n ‚ùåüî¥‚ùåüî¥‚ùåüî¥‚ùåüî¥ BEM-VINDO AO JOGO QUIXO ‚ùåüî¥‚ùåüî¥‚ùåüî¥‚ùåüî¥ \\n\\n\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. üë§ Jogar contra Minimax\")\n",
        "    print(\"2. ü§ñ Jogar contra Q-Learning\")\n",
        "    print(\"3. üß† Treinar Q-Learning\")\n",
        "    print(\"4. üîÑ Q-Learning vs Minimax\")\n",
        "    print(\"5. üìä Avaliar modelos\")\n",
        "    print(\"6. üíæ Carregar modelo Q-Learning\")\n",
        "    print(\"7. ‚ùì Ajuda\")\n",
        "    print(\"8. üö™ Sair\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "def jogar_contra_humano(agente_tipo=\"minimax\", agente_q=None):\n",
        "    # Permite jogar contra um dos agentes\n",
        "    estado = Jogo(jogador='üî¥')\n",
        "    contador_turnos = 0\n",
        "    max_turnos = 100\n",
        "\n",
        "    simbolo_agente = \"‚ùå\"\n",
        "    nome_agente = \"Minimax\" if agente_tipo == \"minimax\" else \"Q-Learning\"\n",
        "\n",
        "    print(f\"\\nüéØ Voc√™ (üî¥) vs {nome_agente} ({simbolo_agente})\")\n",
        "    print(\"üÜò Digite 'ajuda' para ver as regras\")\n",
        "    print(\"üìÑ Digite 'historico' para ver as √∫ltimas jogadas\")\n",
        "    print(\"üîö Digite 'sair' para voltar ao menu\\n\")\n",
        "\n",
        "    while contador_turnos < max_turnos:\n",
        "        estado.imprimir()\n",
        "\n",
        "        if estado.venceu():\n",
        "            vencedor = 'üî¥' if estado.jogador_atual == '‚ùå' else '‚ùå'\n",
        "            print(f\"üèÜ VIT√ìRIA DO JOGADOR {vencedor}! üèÜ\")\n",
        "            estado.historico.imprimir_historico()\n",
        "            break\n",
        "        elif estado.empate():\n",
        "            print(\"ü§ù EMPATE! ü§ù\")\n",
        "            estado.historico.imprimir_historico()\n",
        "            break\n",
        "\n",
        "        if estado.turno() == \"üî¥\":  # Turno do usu√°rio\n",
        "            while True:\n",
        "                try:\n",
        "                    entrada = input(\"Sua jogada (origem destino): \").strip().lower()\n",
        "\n",
        "                    if entrada == \"ajuda\":\n",
        "                        mostrar_ajuda()\n",
        "                        continue\n",
        "                    elif entrada == \"historico\":\n",
        "                        estado.historico.imprimir_historico()\n",
        "                        continue\n",
        "                    elif entrada == \"sair\":\n",
        "                        return\n",
        "\n",
        "                    origem, destino = entrada.split()\n",
        "                    novo_estado = estado.jogar((origem, destino))\n",
        "\n",
        "                    if novo_estado:\n",
        "                        estado.historico.adicionar_jogada(\"üî¥\", origem, destino)\n",
        "                        estado = novo_estado\n",
        "                        break\n",
        "                    else:\n",
        "                        print(\"‚ùå Movimento inv√°lido! Lembre-se das regras do Quixo.\")\n",
        "                except (ValueError, IndexError):\n",
        "                    print(\"‚ö†Ô∏è Formato inv√°lido. Use: 'linha,coluna linha,coluna' (ex: 1,1 5,1)\")\n",
        "        else:  # Turno do agente\n",
        "            print(f\"\\nü§ñ {nome_agente} pensando...\\n\")\n",
        "\n",
        "            if agente_tipo == \"minimax\":\n",
        "                mov = melhor_jogada(estado)\n",
        "            else:  # Q-Learning\n",
        "                if agente_q is None:\n",
        "                    print(\"‚ùå Agente Q-Learning n√£o carregado!\")\n",
        "                    return\n",
        "                mov = agente_q.escolher_acao(estado, modo=\"teste\")\n",
        "\n",
        "            if mov:\n",
        "                origem, destino = mov\n",
        "                print(f\"ü§ñ {nome_agente} move de {origem} para {destino}\\n\")\n",
        "                estado.historico.adicionar_jogada(\"‚ùå\", origem, destino)\n",
        "                estado.ultima_jogada_agente = mov\n",
        "                estado = estado.jogar(mov)\n",
        "            else:\n",
        "                print(f\"{nome_agente} n√£o conseguiu encontrar um movimento v√°lido.\")\n",
        "                break\n",
        "\n",
        "        contador_turnos += 1\n",
        "\n",
        "    if contador_turnos >= max_turnos:\n",
        "        print(\"Jogo encerrado por n√∫mero excessivo de turnos.\")\n",
        "\n",
        "def demonstrar_q_vs_minimax(agente_q, num_jogos=10):\n",
        "    # Demonstra jogos entre Q-Learning e Minimax\n",
        "    print(f\"\\nüî• DEMONSTRA√á√ÉO: Q-Learning vs Minimax ({num_jogos} jogos)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    treinador = TreinadorQLearning(agente_q)\n",
        "\n",
        "    for i in range(num_jogos):\n",
        "        print(f\"\\nüéÆ JOGO {i+1}/{num_jogos}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Alterna quem come√ßa\n",
        "        q_comeca = i % 2 == 0\n",
        "        jogo = Jogo(jogador=\"üî¥\" if q_comeca else \"‚ùå\")\n",
        "        contador = 0\n",
        "        max_jogadas = 30\n",
        "\n",
        "        print(f\"{'Q-Learning (üî¥)' if q_comeca else 'Minimax (üî¥)'} vs {'Minimax (‚ùå)' if q_comeca else 'Q-Learning (‚ùå)'}\")\n",
        "\n",
        "        while not jogo.venceu() and not jogo.empate() and contador < max_jogadas:\n",
        "            jogador_atual = \"Q-Learning\" if ((q_comeca and jogo.turno() == \"üî¥\") or (not q_comeca and jogo.turno() == \"‚ùå\")) else \"Minimax\"\n",
        "\n",
        "            if jogador_atual == \"Q-Learning\":\n",
        "                acao = agente_q.escolher_acao(jogo, modo=\"teste\")\n",
        "            else:\n",
        "                acao = melhor_jogada(jogo, profundidade=2)\n",
        "\n",
        "            if acao:\n",
        "                origem, destino = acao\n",
        "                print(f\"{jogador_atual} ({jogo.turno()}): {origem} ‚Üí {destino}\")\n",
        "                jogo = jogo.jogar(acao)\n",
        "\n",
        "            if jogo is None:\n",
        "                break\n",
        "            contador += 1\n",
        "\n",
        "        # Resultado\n",
        "        if jogo and jogo.venceu():\n",
        "            ultimo_jogador = \"‚ùå\" if jogo.turno() == \"üî¥\" else \"üî¥\"\n",
        "            if (q_comeca and ultimo_jogador == \"üî¥\") or (not q_comeca and ultimo_jogador == \"‚ùå\"):\n",
        "                print(\"üèÜ Q-Learning VENCEU!\")\n",
        "            else:\n",
        "                print(\"üèÜ Minimax VENCEU!\")\n",
        "        else:\n",
        "            print(\"ü§ù EMPATE!\")\n",
        "\n",
        "        if jogo:\n",
        "            jogo.imprimir()\n",
        "\n",
        "        input(\"Pressione Enter para continuar...\")\n",
        "\n",
        "def treinar_interface():\n",
        "    # Interface para treinamento do Q-Learning\n",
        "    print(\"\\nüß† CONFIGURA√á√ÉO DO TREINAMENTO\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    try:\n",
        "        episodios = int(input(\"N√∫mero de epis√≥dios (padr√£o: 5000): \") or \"5000\")\n",
        "        oponente = input(\"Oponente [random/minimax] (padr√£o: random): \").lower() or \"random\"\n",
        "        salvar_freq = int(input(\"Salvar a cada N epis√≥dios (padr√£o: 1000): \") or \"1000\")\n",
        "    except ValueError:\n",
        "        print(\"‚ùå Valores inv√°lidos. Usando configura√ß√µes padr√£o.\")\n",
        "        episodios = 5000\n",
        "        oponente = \"random\"\n",
        "        salvar_freq = 1000\n",
        "\n",
        "    if oponente not in [\"random\", \"minimax\"]:\n",
        "        oponente = \"random\"\n",
        "\n",
        "    print(f\"\\nüöÄ Iniciando treinamento:\")\n",
        "    print(f\"üìä Epis√≥dios: {episodios}\")\n",
        "    print(f\"üéØ Oponente: {oponente}\")\n",
        "    print(f\"üíæ Salvar a cada: {salvar_freq} epis√≥dios\")\n",
        "\n",
        "    confirmar = input(\"\\nContinuar? [s/N]: \").lower()\n",
        "    if confirmar != 's':\n",
        "        return None\n",
        "\n",
        "    # Cria e treina o agente\n",
        "    agente = QLearningAgent(jogador=\"‚ùå\")\n",
        "    treinador = TreinadorQLearning(agente)\n",
        "\n",
        "    try:\n",
        "        treinador.treinar(\n",
        "            num_episodios=episodios,\n",
        "            oponente=oponente,\n",
        "            salvar_a_cada=salvar_freq\n",
        "        )\n",
        "        print(\"‚úÖ Treinamento conclu√≠do com sucesso!\")\n",
        "        return agente\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚èπÔ∏è Treinamento interrompido pelo usu√°rio.\")\n",
        "        agente.salvar_modelo(\"q_learning_parcial.pkl\")\n",
        "        print(\"üíæ Modelo parcial salvo como 'q_learning_parcial.pkl'\")\n",
        "        return agente\n",
        "\n",
        "def avaliar_modelos(agente_q):\n",
        "    # Interface para avalia√ß√£o de modelos\n",
        "    print(\"\\nüìä AVALIA√á√ÉO DE MODELOS\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    try:\n",
        "        num_jogos = int(input(\"N√∫mero de jogos para avalia√ß√£o (padr√£o: 100): \") or \"100\")\n",
        "    except ValueError:\n",
        "        num_jogos = 100\n",
        "\n",
        "    treinador = TreinadorQLearning(agente_q)\n",
        "    resultados = treinador.avaliar_contra_minimax(num_jogos)\n",
        "\n",
        "    # An√°lise estat√≠stica simples\n",
        "    taxa_vitoria_q = (resultados['q_learning'] / resultados['total']) * 100\n",
        "    taxa_vitoria_minimax = (resultados['minimax'] / resultados['total']) * 100\n",
        "\n",
        "    print(f\"\\nüìà AN√ÅLISE ESTAT√çSTICA:\")\n",
        "    print(f\"Taxa de vit√≥ria Q-Learning: {taxa_vitoria_q:.1f}%\")\n",
        "    print(f\"Taxa de vit√≥ria Minimax: {taxa_vitoria_minimax:.1f}%\")\n",
        "\n",
        "    if taxa_vitoria_q > taxa_vitoria_minimax:\n",
        "        print(\"üéØ Q-Learning demonstrou desempenho superior!\")\n",
        "    elif taxa_vitoria_minimax > taxa_vitoria_q:\n",
        "        print(\"üéØ Minimax demonstrou desempenho superior!\")\n",
        "    else:\n",
        "        print(\"ü§ù Desempenho equilibrado entre os algoritmos!\")\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Execu√ß√£o principal do jogo\n",
        "if __name__ == \"__main__\":\n",
        "    agente_qlearning = None\n",
        "\n",
        "    while True:\n",
        "        menu_principal()\n",
        "\n",
        "        try:\n",
        "            opcao = input(\"Escolha uma op√ß√£o: \").strip()\n",
        "\n",
        "            if opcao == '1':\n",
        "                jogar_contra_humano(\"minimax\")\n",
        "\n",
        "            elif opcao == '2':\n",
        "                if agente_qlearning is None:\n",
        "                    print(\"‚ùå Nenhum agente Q-Learning carregado!\")\n",
        "                    print(\"üí° Carregue um modelo (op√ß√£o 6) ou treine um novo (op√ß√£o 3)\")\n",
        "                else:\n",
        "                    jogar_contra_humano(\"qlearning\", agente_qlearning)\n",
        "\n",
        "            elif opcao == '3':\n",
        "                agente_qlearning = treinar_interface()\n",
        "\n",
        "            elif opcao == '4':\n",
        "                if agente_qlearning is None:\n",
        "                    print(\"‚ùå Nenhum agente Q-Learning carregado!\")\n",
        "                    print(\"üí° Carregue um modelo (op√ß√£o 6) ou treine um novo (op√ß√£o 3)\")\n",
        "                else:\n",
        "                    try:\n",
        "                        num = int(input(\"Quantos jogos demonstrar? (padr√£o: 5): \") or \"5\")\n",
        "                        demonstrar_q_vs_minimax(agente_qlearning, num)\n",
        "                    except ValueError:\n",
        "                        demonstrar_q_vs_minimax(agente_qlearning, 5)\n",
        "\n",
        "            elif opcao == '5':\n",
        "                if agente_qlearning is None:\n",
        "                    print(\"‚ùå Nenhum agente Q-Learning carregado!\")\n",
        "                else:\n",
        "                    avaliar_modelos(agente_qlearning)\n",
        "\n",
        "            elif opcao == '6':\n",
        "                nome_arquivo = input(\"Nome do arquivo (padr√£o: q_learning_model.pkl): \").strip()\n",
        "                if not nome_arquivo:\n",
        "                    nome_arquivo = \"q_learning_model.pkl\"\n",
        "\n",
        "                agente_qlearning = QLearningAgent()\n",
        "                if agente_qlearning.carregar_modelo(nome_arquivo):\n",
        "                    print(f\"‚úÖ Modelo carregado com sucesso de {nome_arquivo}\")\n",
        "                    print(f\"üìä Estados na tabela Q: {len(agente_qlearning.q_table)}\")\n",
        "                    print(f\"üéØ Epsilon atual: {agente_qlearning.epsilon:.4f}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Erro ao carregar modelo de {nome_arquivo}\")\n",
        "                    agente_qlearning = None\n",
        "\n",
        "            elif opcao == '7':\n",
        "                mostrar_ajuda()\n",
        "\n",
        "            elif opcao == '8':\n",
        "                print(\"üëã Obrigado por jogar! At√© a pr√≥xima!\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå Op√ß√£o inv√°lida! Escolha um n√∫mero de 1 a 8.\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Programa encerrado pelo usu√°rio.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro inesperado: {e}\")\n",
        "            print(\"üîÑ Retornando ao menu principal...\")\n",
        "\n",
        "        input(\"\\nPressione Enter para continuar...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}